{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Support Vector Machine (SVM)"
      ],
      "metadata": {
        "id": "_BTYCV2D4Y2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Setup and Data Loading**\n",
        "\n",
        "- **Imports**:\n",
        "\n",
        "- - SVC (Support Vector Classifier) from scikit-learn.\n",
        "\n",
        "- - StandardScaler for feature standardization (critical for SVM).\n",
        "\n",
        "- - classification_report for model evaluation.\n",
        "\n",
        "- **Data Loading**:\n",
        "\n",
        "- - Loads magic04.data with particle collision features.\n",
        "\n",
        "- - Converts class labels to binary (1=Gamma, 0=Hadron)."
      ],
      "metadata": {
        "id": "FKHuErgl92dM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkWLLi834H4R"
      },
      "outputs": [],
      "source": [
        "# Setup and Data Loading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "cols = [\"fLength\", \"fWidth\", \"fSize\", \"fConc\", \"fConcl\", \"fAsym\",\n",
        "        \"fM3Long\", \"fM3Trans\", \"fAlpha\", \"fDist\", \"class\"]\n",
        "df = pd.read_csv(\"magic04.data\", names=cols)\n",
        "df[\"class\"] = (df[\"class\"] == \"g\").astype(int)  # Binary: 1=Gamma, 0=Hadron"
      ],
      "metadata": {
        "id": "St5AfYNF4--R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Data Preparation**\n",
        "\n",
        "- **prepare_data() Function**:\n",
        "\n",
        "- - Standardizes features (StandardScaler) to ensure equal influence on the SVM margin.\n",
        "\n",
        "- - Oversampling: Uses RandomOverSampler to balance classes (critical for SVM, which is sensitive to class imbalance).\n",
        "\n",
        "- **Train-Test Split**:\n",
        "\n",
        "- - 60% training, 20% validation, 20% test (shuffled)."
      ],
      "metadata": {
        "id": "L-mBUs0g-ILV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation\n",
        "def prepare_data(df, oversample=False):\n",
        "    X = df[cols[:-1]].values\n",
        "    y = df[cols[-1]].values\n",
        "    X = StandardScaler().fit_transform(X)  # Standardize features\n",
        "    if oversample:\n",
        "        from imblearn.over_sampling import RandomOverSampler\n",
        "        X, y = RandomOverSampler().fit_resample(X, y)\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "WoS0DC6i5IIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "train, valid, test = np.split(df.sample(frac=1), [int(0.6*len(df)), int(0.8*len(df))])\n",
        "X_train, y_train = prepare_data(train, oversample=True)  # Balance classes\n",
        "X_test, y_test = prepare_data(test)"
      ],
      "metadata": {
        "id": "G4HQT_Or5Lx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Training and Evaluation**\n",
        "\n",
        "- **Model Initialization**:\n",
        "\n",
        "- - svm = SVC(kernel='rbf')  # RBF kernel for non-linear boundaries\n",
        "Other kernels: linear, poly, sigmoid.\n",
        "\n",
        "- **Training**:\n",
        "\n",
        "- - svm.fit(X_train, y_train)  # Finds optimal hyperplane\n",
        "\n",
        "- **Prediction & Evaluation**:\n",
        "\n",
        "- - y_pred = svm.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "Reports precision, recall, F1-score, and accuracy."
      ],
      "metadata": {
        "id": "x5xKAl77-aCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Evaluate SVM\n",
        "svm = SVC(kernel='rbf')  # Try kernel='linear', 'poly'\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "WGEKbMYU5N-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Decision Boundary Visualization (2D PCA)**\n",
        "\n",
        "- **PCA Reduction**: Projects features onto 2 principal components for plotting.\n",
        "\n",
        "- **Meshgrid Prediction**: Generates a grid of points and predicts their class using the trained SVM.\n",
        "\n",
        "- **Plot**:\n",
        "\n",
        "- - plt.contourf(xx, yy, Z, alpha=0.3)  # Decision boundary\n",
        "- - plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train)  # Data points\n",
        "\n",
        "Shows how the non-linear (RBF) boundary separates classes in reduced space."
      ],
      "metadata": {
        "id": "QzBZ1wYT-1QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Decision Boundary (2D PCA Projection)\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# Step 1: Reduce features to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Step 2: Retrain SVM on 2D data (for visualization)\n",
        "model_pca = SVC(kernel='rbf').fit(X_train_pca, y_train)\n",
        "\n",
        "# Step 3: Create a meshgrid for decision boundary\n",
        "x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
        "y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = model_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Step 4: Plot decision boundary and data points\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Paired)\n",
        "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train,\n",
        "            edgecolor='k', s=50, cmap=plt.cm.Paired)\n",
        "plt.title(\"SVM Decision Boundary (PCA-Reduced Data)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.colorbar(label=\"Class (0=Hadron, 1=Gamma)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1iZ5-3Cb5eR_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}