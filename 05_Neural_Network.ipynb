{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Network (TensorFlow/Keras)"
      ],
      "metadata": {
        "id": "z7Hj2ePG60s_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo pip3 install keras"
      ],
      "metadata": {
        "id": "4YSFpK3h_zcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: **Setup and Data Loading**\n",
        "\n",
        "- **Imports**:\n",
        "\n",
        "- - tensorflow and keras for building/training the neural network.\n",
        "\n",
        "- - StandardScaler for feature standardization.\n",
        "\n",
        "- - RandomOverSampler to handle class imbalance.\n",
        "\n",
        "- **Data Loading**:\n",
        "\n",
        "- Loads the magic04.data file with particle collision features.\n",
        "\n",
        "- Converts the class column to binary (1=Gamma, 0=Hadron)."
      ],
      "metadata": {
        "id": "qA2OvCbtCklX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f75QOI376zkw"
      },
      "outputs": [],
      "source": [
        "# Setup and Data Loading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "cols = [\"fLength\", \"fWidth\", \"fSize\", \"fConc\", \"fConcl\", \"fAsym\",\n",
        "        \"fM3Long\", \"fM3Trans\", \"fAlpha\", \"fDist\", \"class\"]\n",
        "df = pd.read_csv(\"magic04.data\", names=cols)\n",
        "df[\"class\"] = (df[\"class\"] == \"g\").astype(int)  # Binary: 1=Gamma, 0=Hadron"
      ],
      "metadata": {
        "id": "N_tDCCT57Cy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: **Data Preparation**\n",
        "\n",
        "- **prepare_data() Function**:\n",
        "\n",
        "- Standardizes features using StandardScaler() (critical for neural networks).\n",
        "\n",
        "- If oversample=True, applies RandomOverSampler to balance classes."
      ],
      "metadata": {
        "id": "d4kGHRE_C4CL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation\n",
        "def prepare_data(df, oversample=False):\n",
        "    X = df[cols[:-1]].values\n",
        "    y = df[cols[-1]].values\n",
        "    X = StandardScaler().fit_transform(X)  # Standardize features\n",
        "    if oversample:\n",
        "        from imblearn.over_sampling import RandomOverSampler\n",
        "        X, y = RandomOverSampler().fit_resample(X, y)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "e8BcxjaL7Et4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: **Data Splitting**\n",
        "\n",
        "- **Splits Data**:\n",
        "\n",
        "- - 60% training, 20% validation, 20% test (shuffled).\n",
        "\n",
        "- - Training set is oversampled; validation/test sets use original distribution."
      ],
      "metadata": {
        "id": "xUUhuLdpDJ5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Splitting - FIXED VERSION\n",
        "# Split into train (60%), validation (20%), and test (20%)\n",
        "train, valid, test = np.split(df.sample(frac=1), [int(0.6*len(df)), int(0.8*len(df))])\n",
        "X_train, y_train = prepare_data(train, oversample=True)  # Balance classes\n",
        "X_valid, y_valid = prepare_data(valid)                   # Validation set\n",
        "X_test, y_test = prepare_data(test)                      # Test set"
      ],
      "metadata": {
        "id": "xALQA3Ek7Guy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: **Neural Network Model**\n",
        "\n",
        "- **Architecture**:\n",
        "\n",
        "- Input Layer: 10 features â†’ 32 neurons (ReLU activation).\n",
        "\n",
        "- Hidden Layer: 32 neurons (ReLU).\n",
        "\n",
        "- Output Layer: 1 neuron (sigmoid for binary probability).\n",
        "\n",
        "- **Compilation**:\n",
        "\n",
        "- - Optimizer: Adam (adaptive learning rate).\n",
        "\n",
        "- - Loss: binary_crossentropy (for binary classification).\n",
        "\n",
        "- - Metric: Accuracy."
      ],
      "metadata": {
        "id": "kCXFHmG6DV_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Model\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(32, activation='relu', input_shape=(10,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "uNoGxBqu7I2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: **Training with Validation**\n",
        "\n",
        "- **Training**:\n",
        "\n",
        "- - Runs for 50 epochs (passes through the dataset).\n",
        "\n",
        "- - Batch Size: 32 (samples processed before weight updates).\n",
        "\n",
        "- - Validation Data: Monitored to detect overfitting."
      ],
      "metadata": {
        "id": "1vUtml4VDsqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with Validation\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(X_valid, y_valid),  # Now properly defined\n",
        "                    verbose=1)\n"
      ],
      "metadata": {
        "id": "46u_4rEc7NFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6: **Evaluation**\n",
        "\n",
        "- **Test Performance**:\n",
        "\n",
        "- - Computes loss and accuracy on the unseen test set.\n",
        "\n",
        "- - Example output: Test accuracy: 0.8724."
      ],
      "metadata": {
        "id": "n1aJb1liD8t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"\\nTest accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "4XS6D7f08xX3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}