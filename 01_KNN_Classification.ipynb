{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#K-Nearest Neighbors (KNN) Classification"
      ],
      "metadata": {
        "id": "uKy2DM3wyOKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Setup and Data Loading**\n",
        "\n",
        "---\n",
        "\n",
        "*   **Imports**: Libraries like pandas, numpy, and scikit-learn are imported for data handling, preprocessing, and modeling.\n",
        "*   **Data Loading**: The dataset magic04.data is loaded with columns describing features of particle collisions (e.g., fLength, fWidth) and a binary class (1=Gamma, 0=Hadron). The target is converted to binary integers for classification.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_SqyRYUk_7vL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBCnrpIkyLm2"
      },
      "outputs": [],
      "source": [
        "# Setup and Data Loading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "cols = [\"fLength\", \"fWidth\", \"fSize\", \"fConc\", \"fConcl\", \"fAsym\",\n",
        "        \"fM3Long\", \"fM3Trans\", \"fAlpha\", \"fDist\", \"class\"]\n",
        "df = pd.read_csv(\"magic04.data\", names=cols)\n",
        "df[\"class\"] = (df[\"class\"] == \"g\").astype(int)  # Binary: 1=Gamma, 0=Hadron\n"
      ],
      "metadata": {
        "id": "E68Rpj-JyYzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Data Preparation**\n",
        "---\n",
        "\n",
        "*  **prepare_data() Function:**\n",
        "\n",
        "*   - **Standardization**: Features are standardized using StandardScaler() to ensure equal weight in distance calculations.\n",
        "*   - **Oversampling**: If oversample=True, the minority class is oversampled (using RandomOverSampler) to address class imbalance.\n",
        "\n",
        "*   **Train-Validation**\n",
        "- - **Test Split**: Data is shuffled and split into 60% training, 20% validation, and 20% test sets. The training set is oversampled for balance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tmpl8ahSAdYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation\n",
        "def prepare_data(df, oversample=False):\n",
        "    X = df[cols[:-1]].values\n",
        "    y = df[cols[-1]].values\n",
        "    X = StandardScaler().fit_transform(X)  # Standardize features\n",
        "    if oversample:\n",
        "        from imblearn.over_sampling import RandomOverSampler\n",
        "        X, y = RandomOverSampler().fit_resample(X, y)\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "8nPknuhhyfTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "train, valid, test = np.split(df.sample(frac=1), [int(0.6*len(df)), int(0.8*len(df))])\n",
        "X_train, y_train = prepare_data(train, oversample=True)  # Balance classes\n",
        "X_test, y_test = prepare_data(test)\n"
      ],
      "metadata": {
        "id": "ZHJNMUhYynSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **KNN Training and Evaluation**\n",
        "\n",
        "---\n",
        "\n",
        "- **Model Initialization**: KNeighborsClassifier(n_neighbors=5) creates a KNN model with *k*=5.\n",
        "\n",
        "- **Training**: knn.fit() stores the standardized training data.\n",
        "\n",
        "- **Prediction & Evaluation**: Predictions on the test set (X_test) are evaluated using classification_report, which outputs precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "tFDJMKDzBkcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Evaluate KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=5)  # Try k=1, 3, 5, etc.\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "iEDZcHWdysZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Decision Boundary Visualization (2D PCA Projection)**\n",
        "\n",
        "- **Dimensionality Reduction**: PCA reduces features to 2 principal components for visualization.\n",
        "\n",
        "- **Meshgrid Creation**: A grid of points is generated to cover the feature space, and KNN predicts class labels for each point.\n",
        "\n",
        "- **Plotting**:\n",
        "\n",
        "- - contourf() draws the decision boundary based on grid predictions.\n",
        "\n",
        "- - scatter() plots the PCA-reduced training points, colored by class.\n",
        "\n",
        "- - The plot helps visualize how KNN classifies regions based on neighbor votes."
      ],
      "metadata": {
        "id": "yyTqr-2kCebb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Decision Boundary (2D PCA Projection)\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Reduce features to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Step 2: Retrain KNN on 2D data (for visualization)\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "# Step 3: Create a meshgrid for decision boundary\n",
        "x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
        "y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = knn_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Step 4: Plot decision boundary and data points\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Paired)\n",
        "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train,\n",
        "            edgecolor='k', s=50, cmap=plt.cm.Paired)\n",
        "plt.title(\"KNN Decision Boundary (PCA-Reduced Data)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.colorbar(label=\"Class (0=Hadron, 1=Gamma)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kEVkihwuzdsr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}